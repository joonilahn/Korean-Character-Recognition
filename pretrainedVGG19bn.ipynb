{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os, glob, argparse\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy.ndimage import imread\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangulDataset(Dataset):\n",
    "    \"\"\"Hangul Handwritten dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, subroot='char_data', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.subroot = subroot\n",
    "        self.transform = transform\n",
    "        self.classlist = os.listdir(os.path.join(root_dir, self.subroot))\n",
    "        self.targets = []\n",
    "        self.filenames = []\n",
    "        self.targetdict = {}\n",
    "        for i, label in enumerate(self.classlist):\n",
    "            files = glob.glob(\n",
    "                        os.path.join(self.root_dir, self.subroot, label) + '/*')\n",
    "            self.filenames += files\n",
    "            self.targets += [i] * len(files)\n",
    "            self.targetdict[i] = int(label, 16)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        target = self.targets[idx]\n",
    "        sample = imread(img_name, mode='L')\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return (sample, target)\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or tuple): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (self.output_size, self.output_size),\n",
    "                               mode='reflect')\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "\n",
    "        blkidx = np.where(img > 0.7)\n",
    "        img[blkidx] = 0\n",
    "        return img\n",
    "    \n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return image\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        dims = sample.shape\n",
    "        if len(dims) == 2:\n",
    "            image = np.expand_dims(sample, 0)\n",
    "        else:\n",
    "            image = sample.transpose((2, 0, 1))\n",
    "        sampletensor = torch.from_numpy(image)\n",
    "        return sampletensor.type(torch.FloatTensor)\n",
    "    \n",
    "# class CleanImage(object):\n",
    "#     \"\"\"Delete background pixels to clean up images\"\"\"\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         blkidx = np.where(sample > 0.7)\n",
    "#         sample[blkidx] = 0\n",
    "#         return sample\n",
    "    \n",
    "class ObjectCrop(object):\n",
    "    \"\"\"Detect centre of character and crop unnecessary background\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "            \n",
    "    def __call__(self, sample):\n",
    "        h, w = sample.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        # Get centre of character\n",
    "        whiteidx = np.where(sample > 0)\n",
    "        h_centre = whiteidx[0].mean().astype(int)\n",
    "        w_centre = whiteidx[1].mean().astype(int)\n",
    "        centrepixel = (h_centre, w_centre)\n",
    "        \n",
    "        h_start = h_centre - (new_h // 2)\n",
    "        h_end = h_start + new_h\n",
    "        w_start = w_centre - (new_w // 2)\n",
    "        w_end = w_start + new_w\n",
    "\n",
    "        if h_start < 0:   # If new index is less than 0\n",
    "            h_start = 0\n",
    "            h_end = new_h - 1\n",
    "        elif h_end > h:   # If new index is larger than original height\n",
    "            h_end = h\n",
    "            h_start = h - new_h\n",
    "        if w_start < 0:\n",
    "            w_start = 0\n",
    "            w_end = new_w - 1\n",
    "        elif w_end > w:\n",
    "            w_end = w\n",
    "            w_start = w - new_w\n",
    "            \n",
    "        image = sample[h_start:h_end,\n",
    "                      w_start:w_end]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hangul_dataset = HangulDataset('set01', transform=Rescale(256))\n",
    "transformed_dataset = HangulDataset(root_dir='set01',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    Rescale(256),\n",
    "                                    ObjectCrop(224),\n",
    "                                    ToTensor(),    \n",
    "                                    transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n",
    "#                                     transforms.Normalize([0.97,0.97,0.97], [0.09,0.09,0.09])\n",
    "                                           ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=100,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMabC/weX41oMGOwI4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8b3d57e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(transformed_dataset[0][0].numpy().transpose(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor sizes at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/TH/generic/THTensorMath.c:2709",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b1640ff1f38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch from dataloader'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batched\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     raise TypeError((\"batch must contain tensors, numbers, dicts or lists; found {}\"\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     raise TypeError((\"batch must contain tensors, numbers, dicts or lists; found {}\"\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_py3/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(sequence, dim, out)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor sizes at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/TH/generic/THTensorMath.c:2709"
     ]
    }
   ],
   "source": [
    "def show_batch(sample_batched):\n",
    "    \"\"\"Show images for a batch of samples.\"\"\"\n",
    "    batch_size = len(sample_batched)\n",
    "    im_size = sample_batched.size(2)\n",
    "\n",
    "    grid = utils.make_grid(sample_batched)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    plt.title('Batch from dataloader')\n",
    "\n",
    "for i_batch, (sample_batched, target_batched) in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched.size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model = models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create custom VGG19_bn\n",
    "num_classes = 2350\n",
    "class CustomVGG19bn(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomVGG19bn, self).__init__()\n",
    "        self.features = nn.Sequential(*list(pretrained_model.features.children()))\n",
    "        self.classifier = nn.Sequential(\n",
    "            *[list(pretrained_model.classifier.children())[i] for i in range(6)],\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 25088)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# load custom model\n",
    "model = CustomVGG19bn(num_classes=2350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer_idx, param in enumerate(model.classifier.parameters()):\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([4096, 25088]), torch.Size([4096]), torch.Size([4096, 4096]), torch.Size([4096]), torch.Size([2350, 4096]), torch.Size([2350])]\n"
     ]
    }
   ],
   "source": [
    "# unfrozen_weights = filter(lambda x: x.requires_grad, model.parameters())\n",
    "# print(list(map(lambda x: x.size(), unfrozen_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs).float().cuda()\n",
    "                    labels = Variable(labels).long().cuda()\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs).float(), Variable(labels).long()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # print every 10 iterations\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print('Epoch: {0:}/{1:}, Iterations: {2:}/{3:}, Training loss: {4:6.2f}'.\n",
    "                     format(epoch, num_epochs, i, len(dataloaders[phase]), loss.data[0]))\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train, test data split\n",
    "num_data = len(hangul_dataset.filenames)\n",
    "indices = list(range(num_data))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_size = 0.20\n",
    "test_size = 0.20\n",
    "test_split = int(np.floor(test_size * num_data))\n",
    "val_split = test_split + int(np.floor(val_size * num_data))\n",
    "num_train = num_data - val_split - test_split\n",
    "train_idx, val_idx, test_idx = indices[val_split:], indices[test_split:val_split] , indices[:test_split]\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    " # Define sampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Train, test dataset loader\n",
    "train_loader = DataLoader(transformed_dataset, \n",
    "                        batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "val_loader = DataLoader(transformed_dataset, \n",
    "                        batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "test_loader = DataLoader(transformed_dataset, \n",
    "                        batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "dataloaders = {'train':train_loader, 'val':val_loader, 'test':test_loader}\n",
    "\n",
    " # use gpu if cuda is available\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-277c9c8400cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=num_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a296d4038eb2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3",
   "language": "python",
   "name": "pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
